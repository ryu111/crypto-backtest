#!/usr/bin/env python3
"""
æ•´åˆåŸºæº–æ¸¬è©¦åŸ·è¡Œè…³æœ¬

æä¾›å‘½ä»¤è¡Œä»‹é¢ä¾†åŸ·è¡Œæ‰€æœ‰åŸºæº–æ¸¬è©¦ï¼Œæ”¯æ´ï¼š
- DataFrame æ“ä½œæ•ˆèƒ½ï¼ˆPandas vs Polarsï¼‰
- å›æ¸¬å¼•æ“æ•ˆèƒ½
- GPU æ‰¹é‡å„ªåŒ–æ•ˆèƒ½

ä½¿ç”¨æ–¹å¼ï¼š
    python benchmarks/run_all_benchmarks.py --quick
    python benchmarks/run_all_benchmarks.py --data-sizes 10000 50000
    python benchmarks/run_all_benchmarks.py --skip-gpu
"""

import argparse
import sys
import time
import traceback
from pathlib import Path
from typing import Dict, List, Callable
from datetime import datetime

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.benchmark.runners import (
    DataFrameRunner,
    EngineRunner,
    GPURunner,
    BenchmarkReport
)


# ============================================================================
# å¸¸æ•¸
# ============================================================================

SECONDS_PER_MINUTE = 60
SECONDS_PER_HOUR = 3600
SEPARATOR_WIDTH = 70


# ============================================================================
# é€²åº¦é¡¯ç¤º
# ============================================================================

class ProgressTracker:
    """é€²åº¦è¿½è¹¤å™¨"""

    def __init__(self, total_tests: int, verbose: bool = False):
        """
        åˆå§‹åŒ–é€²åº¦è¿½è¹¤å™¨

        Args:
            total_tests: ç¸½æ¸¬è©¦æ•¸é‡
            verbose: æ˜¯å¦è©³ç´°è¼¸å‡º
        """
        self.total_tests = total_tests
        self.verbose = verbose
        self.current = 0
        self.start_time = time.time()
        self.test_times: List[float] = []
        self._test_start: float = 0.0  # åˆå§‹åŒ–é¿å… AttributeError

    def start_test(self, test_name: str):
        """é–‹å§‹æ¸¬è©¦"""
        self.current += 1
        percentage = (self.current / self.total_tests) * 100

        elapsed = time.time() - self.start_time
        if self.current > 1 and self.test_times:
            avg_time = sum(self.test_times) / len(self.test_times)
            remaining_tests = self.total_tests - self.current
            eta_seconds = avg_time * remaining_tests
            eta_str = format_time(eta_seconds)
        else:
            eta_str = "calculating..."

        print(f"\n[{self.current}/{self.total_tests}] ({percentage:.1f}%) {test_name}")
        print(f"  Elapsed: {format_time(elapsed)} | ETA: {eta_str}")

        self._test_start = time.time()

    def end_test(self, success: bool = True):
        """
        çµæŸæ¸¬è©¦

        Args:
            success: æ¸¬è©¦æ˜¯å¦æˆåŠŸï¼Œå¤±æ•—æ™‚ä¸è¨ˆå…¥ ETA è¨ˆç®—
        """
        if self._test_start == 0.0:
            return  # é˜²ç¦¦æ€§æª¢æŸ¥

        test_time = time.time() - self._test_start

        # åªæœ‰æˆåŠŸçš„æ¸¬è©¦æ‰è¨ˆå…¥ ETA è¨ˆç®—
        if success:
            self.test_times.append(test_time)

        if self.verbose:
            status = "âœ“" if success else "âœ—"
            print(f"  {status} Completed in {format_time(test_time)}")


def format_time(seconds: float) -> str:
    """
    æ ¼å¼åŒ–æ™‚é–“

    Args:
        seconds: ç§’æ•¸

    Returns:
        æ ¼å¼åŒ–çš„æ™‚é–“å­—ä¸²
    """
    if seconds < SECONDS_PER_MINUTE:
        return f"{seconds:.1f}s"
    elif seconds < SECONDS_PER_HOUR:
        minutes = int(seconds // SECONDS_PER_MINUTE)
        secs = int(seconds % SECONDS_PER_MINUTE)
        return f"{minutes}m {secs}s"
    else:
        hours = int(seconds // SECONDS_PER_HOUR)
        minutes = int((seconds % SECONDS_PER_HOUR) // SECONDS_PER_MINUTE)
        return f"{hours}h {minutes}m"


# ============================================================================
# æ¸¬è©¦åŸ·è¡Œå™¨
# ============================================================================

class BenchmarkExecutor:
    """åŸºæº–æ¸¬è©¦åŸ·è¡Œå™¨"""

    def __init__(
        self,
        data_sizes: List[int],
        batch_sizes: List[int],
        output_dir: Path,
        skip_gpu: bool = False,
        verbose: bool = False
    ):
        """
        åˆå§‹åŒ–åŸ·è¡Œå™¨

        Args:
            data_sizes: è³‡æ–™å¤§å°åˆ—è¡¨
            batch_sizes: GPU æ‰¹æ¬¡å¤§å°åˆ—è¡¨
            output_dir: å ±å‘Šè¼¸å‡ºç›®éŒ„
            skip_gpu: æ˜¯å¦è·³é GPU æ¸¬è©¦
            verbose: è©³ç´°è¼¸å‡º
        """
        self.data_sizes = data_sizes
        self.batch_sizes = batch_sizes
        self.output_dir = output_dir
        self.skip_gpu = skip_gpu
        self.verbose = verbose

        self.reports: Dict[str, BenchmarkReport] = {}
        self.errors: Dict[str, Exception] = {}

        # å»ºç«‹è¼¸å‡ºç›®éŒ„
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def run(self) -> Dict[str, BenchmarkReport]:
        """
        åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦

        Returns:
            æ¸¬è©¦å ±å‘Šå­—å…¸
        """
        # è¨ˆç®—ç¸½æ¸¬è©¦æ•¸
        total_tests = self._count_tests()

        print("=" * SEPARATOR_WIDTH)
        print("ğŸš€ åˆç´„äº¤æ˜“å›æ¸¬ç³»çµ± - æ•ˆèƒ½åŸºæº–æ¸¬è©¦")
        print("=" * SEPARATOR_WIDTH)
        print(f"è³‡æ–™å¤§å°: {self.data_sizes}")
        print(f"æ‰¹æ¬¡å¤§å°: {self.batch_sizes}")
        print(f"è¼¸å‡ºç›®éŒ„: {self.output_dir}")
        print(f"ç¸½æ¸¬è©¦æ•¸: {total_tests}")
        print("=" * SEPARATOR_WIDTH)

        tracker = ProgressTracker(total_tests, verbose=self.verbose)

        # 1. DataFrame æ“ä½œæ¸¬è©¦
        self._run_dataframe_tests(tracker)

        # 2. å›æ¸¬å¼•æ“æ¸¬è©¦
        self._run_engine_tests(tracker)

        # 3. GPU æ¸¬è©¦
        if not self.skip_gpu:
            self._run_gpu_tests(tracker)

        # å„²å­˜å ±å‘Š
        self._save_reports()

        # é¡¯ç¤ºç¸½çµ
        self._print_summary()

        return self.reports

    def _count_tests(self) -> int:
        """è¨ˆç®—ç¸½æ¸¬è©¦æ•¸"""
        count = 0

        # DataFrame æ¸¬è©¦ï¼š3 ç¨®æ“ä½œ
        count += 3

        # å›æ¸¬å¼•æ“æ¸¬è©¦ï¼š1 ç¨®
        count += 1

        # GPU æ¸¬è©¦ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if not self.skip_gpu:
            count += 1

        return count

    def _run_single_test(
        self,
        tracker: ProgressTracker,
        test_key: str,
        test_name: str,
        test_func: Callable,
        *args
    ):
        """
        åŸ·è¡Œå–®ä¸€æ¸¬è©¦

        Args:
            tracker: é€²åº¦è¿½è¹¤å™¨
            test_key: å ±å‘Šå­—å…¸çš„ key
            test_name: é¡¯ç¤ºåç¨±
            test_func: æ¸¬è©¦å‡½æ•¸
            *args: å‚³å…¥æ¸¬è©¦å‡½æ•¸çš„åƒæ•¸
        """
        tracker.start_test(test_name)
        try:
            self.reports[test_key] = test_func(*args)
            tracker.end_test(success=True)
            if self.verbose:
                print(self.reports[test_key].summary())
        except Exception as e:
            self.errors[test_key] = e
            print(f"  âŒ Error: {e}")
            if self.verbose:
                traceback.print_exc()
            tracker.end_test(success=False)

    def _run_dataframe_tests(self, tracker: ProgressTracker):
        """åŸ·è¡Œ DataFrame æ“ä½œæ¸¬è©¦"""
        print("\n" + "=" * SEPARATOR_WIDTH)
        print("ğŸ“Š DataFrame æ“ä½œæ•ˆèƒ½æ¸¬è©¦")
        print("=" * SEPARATOR_WIDTH)

        df_runner = DataFrameRunner()

        # Rolling Mean
        self._run_single_test(
            tracker, "rolling_mean", "Rolling Mean",
            df_runner.benchmark_rolling_mean, self.data_sizes
        )

        # Where
        self._run_single_test(
            tracker, "where", "Where (Conditional Selection)",
            df_runner.benchmark_where, self.data_sizes
        )

        # EWM
        self._run_single_test(
            tracker, "ewm", "EWM (Exponential Weighted Mean)",
            df_runner.benchmark_ewm, self.data_sizes
        )

    def _run_engine_tests(self, tracker: ProgressTracker):
        """åŸ·è¡Œå›æ¸¬å¼•æ“æ¸¬è©¦"""
        print("\n" + "=" * SEPARATOR_WIDTH)
        print("âš™ï¸ å›æ¸¬å¼•æ“æ•ˆèƒ½æ¸¬è©¦")
        print("=" * SEPARATOR_WIDTH)

        engine_runner = EngineRunner()

        self._run_single_test(
            tracker, "backtest", "Backtest Engine",
            engine_runner.benchmark_backtest, self.data_sizes
        )

    def _run_gpu_tests(self, tracker: ProgressTracker):
        """åŸ·è¡Œ GPU æ¸¬è©¦"""
        print("\n" + "=" * SEPARATOR_WIDTH)
        print("ğŸ® GPU æ‰¹é‡å„ªåŒ–æ•ˆèƒ½æ¸¬è©¦")
        print("=" * SEPARATOR_WIDTH)

        gpu_runner = GPURunner()
        print(f"å¯ç”¨å¾Œç«¯: {gpu_runner.available_backends}")

        if len(gpu_runner.available_backends) > 1:  # ä¸åª CPU
            self._run_single_test(
                tracker, "gpu_batch", "GPU Batch Optimization",
                gpu_runner.benchmark_batch_optimization, self.batch_sizes
            )
        else:
            print("âš ï¸ ç„¡ GPU å¾Œç«¯å¯ç”¨ï¼Œè·³é GPU æ¸¬è©¦")

    def _save_reports(self):
        """å„²å­˜å ±å‘Š"""
        print("\n" + "=" * SEPARATOR_WIDTH)
        print("ğŸ’¾ å„²å­˜å ±å‘Š")
        print("=" * SEPARATOR_WIDTH)

        for name, report in self.reports.items():
            # Markdown
            md_file = self.output_dir / f"{name}_report.md"
            try:
                with open(md_file, 'w', encoding='utf-8') as f:
                    f.write(report.to_markdown())
                print(f"  âœ“ {md_file}")
            except Exception as e:
                print(f"  âŒ Failed to save {md_file}: {e}")

            # JSON
            json_file = self.output_dir / f"{name}_report.json"
            try:
                with open(json_file, 'w', encoding='utf-8') as f:
                    f.write(report.to_json())
                print(f"  âœ“ {json_file}")
            except Exception as e:
                print(f"  âŒ Failed to save {json_file}: {e}")

        # å„²å­˜ç¸½çµå ±å‘Š
        self._save_summary_report()

    def _save_summary_report(self):
        """å„²å­˜ç¸½çµå ±å‘Š"""
        summary_file = self.output_dir / "summary.md"

        try:
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write("# åŸºæº–æ¸¬è©¦ç¸½çµå ±å‘Š\n\n")
                f.write(f"**åŸ·è¡Œæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

                f.write("## é…ç½®\n\n")
                f.write(f"- **è³‡æ–™å¤§å°**: {self.data_sizes}\n")
                f.write(f"- **æ‰¹æ¬¡å¤§å°**: {self.batch_sizes}\n")
                f.write(f"- **è·³é GPU**: {self.skip_gpu}\n\n")

                f.write("## æ¸¬è©¦çµæœ\n\n")
                f.write(f"- **æˆåŠŸ**: {len(self.reports)}\n")
                f.write(f"- **å¤±æ•—**: {len(self.errors)}\n\n")

                if self.errors:
                    f.write("## éŒ¯èª¤\n\n")
                    for name, error in self.errors.items():
                        f.write(f"- **{name}**: {error}\n")
                    f.write("\n")

                f.write("## å ±å‘Šæª”æ¡ˆ\n\n")
                for name in self.reports.keys():
                    f.write(f"- [{name}_report.md](./{name}_report.md)\n")
                    f.write(f"- [{name}_report.json](./{name}_report.json)\n")

            print(f"  âœ“ {summary_file}")

        except Exception as e:
            print(f"  âŒ Failed to save summary: {e}")

    def _print_summary(self):
        """é¡¯ç¤ºç¸½çµ"""
        print("\n" + "=" * SEPARATOR_WIDTH)
        print("ğŸ“‹ æ¸¬è©¦ç¸½çµ")
        print("=" * SEPARATOR_WIDTH)
        print(f"æˆåŠŸ: {len(self.reports)}")
        print(f"å¤±æ•—: {len(self.errors)}")

        if self.errors:
            print("\nå¤±æ•—çš„æ¸¬è©¦:")
            for name, error in self.errors.items():
                print(f"  âŒ {name}: {error}")

        print(f"\nâœ… å ±å‘Šå·²å„²å­˜è‡³ {self.output_dir}")
        print("=" * SEPARATOR_WIDTH)


# ============================================================================
# å‘½ä»¤è¡Œä»‹é¢
# ============================================================================

def parse_args():
    """è§£æå‘½ä»¤è¡Œåƒæ•¸"""
    parser = argparse.ArgumentParser(
        description="åˆç´„äº¤æ˜“å›æ¸¬ç³»çµ±æ•ˆèƒ½åŸºæº–æ¸¬è©¦",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  # å¿«é€Ÿæ¸¬è©¦ï¼ˆå°æ•¸æ“šé‡ï¼‰
  python benchmarks/run_all_benchmarks.py --quick

  # è‡ªè¨‚è³‡æ–™å¤§å°
  python benchmarks/run_all_benchmarks.py --data-sizes 10000 50000 100000

  # è·³é GPU æ¸¬è©¦
  python benchmarks/run_all_benchmarks.py --skip-gpu

  # è©³ç´°è¼¸å‡º
  python benchmarks/run_all_benchmarks.py --verbose
        """
    )

    parser.add_argument(
        '--data-sizes',
        type=int,
        nargs='+',
        default=[10000, 50000, 100000],
        help='è³‡æ–™å¤§å°åˆ—è¡¨ (é è¨­: 10000 50000 100000)'
    )

    parser.add_argument(
        '--batch-sizes',
        type=int,
        nargs='+',
        default=[10, 50, 100],
        help='GPU æ‰¹æ¬¡å¤§å°åˆ—è¡¨ (é è¨­: 10 50 100)'
    )

    parser.add_argument(
        '--output-dir',
        type=Path,
        default=Path('benchmark_results'),
        help='å ±å‘Šè¼¸å‡ºç›®éŒ„ (é è¨­: benchmark_results)'
    )

    parser.add_argument(
        '--skip-gpu',
        action='store_true',
        help='è·³é GPU æ¸¬è©¦'
    )

    parser.add_argument(
        '--quick',
        action='store_true',
        help='å¿«é€Ÿæ¸¬è©¦æ¨¡å¼ï¼ˆå°æ•¸æ“šé‡ï¼‰'
    )

    parser.add_argument(
        '--verbose',
        action='store_true',
        help='è©³ç´°è¼¸å‡º'
    )

    return parser.parse_args()


def main():
    """ä¸»å‡½æ•¸"""
    args = parse_args()

    # å¿«é€Ÿæ¨¡å¼
    if args.quick:
        args.data_sizes = [1000, 5000]
        args.batch_sizes = [10, 20]

    # å»ºç«‹åŸ·è¡Œå™¨
    executor = BenchmarkExecutor(
        data_sizes=args.data_sizes,
        batch_sizes=args.batch_sizes,
        output_dir=args.output_dir,
        skip_gpu=args.skip_gpu,
        verbose=args.verbose
    )

    # åŸ·è¡Œæ¸¬è©¦
    start_time = time.time()
    _ = executor.run()  # å ±å‘Šå·²å„²å­˜åˆ°æª”æ¡ˆ
    total_time = time.time() - start_time

    print(f"\nâ±ï¸ ç¸½åŸ·è¡Œæ™‚é–“: {format_time(total_time)}")

    # è¿”å› exit code
    exit_code = 0 if len(executor.errors) == 0 else 1
    return exit_code


if __name__ == "__main__":
    sys.exit(main())
